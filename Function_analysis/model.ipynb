{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def split_json_file(input_file, output_dir, chunk_size):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        chunk_number = 1\n",
    "        while True:\n",
    "            # Read lines in chunks\n",
    "            lines = [file.readline() for _ in range(chunk_size)]\n",
    "            if not any(lines):\n",
    "                break  # Exit loop if no more lines\n",
    "\n",
    "            # Filter out empty lines\n",
    "            lines = [line for line in lines if line.strip()]\n",
    "\n",
    "            # Parse each line as JSON and write to a new file\n",
    "            data = [json.loads(line) for line in lines]\n",
    "            output_file = os.path.join(output_dir, f'chunk_{chunk_number}.json')\n",
    "            with open(output_file, 'w') as out_file:\n",
    "                json.dump(data, out_file, indent=4)\n",
    "            \n",
    "            chunk_number += 1\n",
    "\n",
    "# Example usag\n",
    "input_file = 'function_data.json'\n",
    "output_dir = 'output_chunks'\n",
    "chunk_size = 100  # Adjust the chunk size as needed\n",
    "\n",
    "split_json_file(input_file, output_dir, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1703 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, None, 512)\n",
      "Target shape: (None, None)\n",
      "Actual input shape: (1, 4, 512)\n",
      "tf.Tensor(\n",
      "[[[    0 50117 50117 ...  1437  1437   403]\n",
      "  [  230  3808 21770 ... 38323  5457 31245]\n",
      "  [ 6972 10463  1215 ...  1437  1437  1437]\n",
      "  [48565 23770  2562 ...     1     1     1]]], shape=(1, 4, 512), dtype=int32)\n",
      "Actual target shape: (1, 13)\n",
      "tf.Tensor(\n",
      "[[    0  1215 16993  1182  6634  1215   438 38914 29015   176 11828 10887\n",
      "      2]], shape=(1, 13), dtype=int32)\n",
      "Number of batches: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinay\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py:360: UserWarning: `build()` was called on layer 'sequence_encoder_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_14                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50265</span>)               │      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,918,105</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15 (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_14                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_41 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m525,312\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50265\u001b[0m)               │      \u001b[38;5;34m12,918,105\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,443,417</span> (51.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,443,417\u001b[0m (51.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,443,417</span> (51.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,443,417\u001b[0m (51.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mInvalid dtype: NoneType\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, None, 512), dtype=int32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 136\u001b[0m\n\u001b[0;32m    133\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    134\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m--> 136\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(dataset, epochs\u001b[38;5;241m=\u001b[39mepochs, steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\variables.py:495\u001b[0m, in \u001b[0;36mstandardize_dtype\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m    492\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mALLOWED_DTYPES:\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mInvalid dtype: NoneType\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, None, 512), dtype=int32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def read_functions_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    content = content.strip()\n",
    "    \n",
    "    if content.startswith('[') and content.endswith(']'):\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "            for item in data:\n",
    "                yield item\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON array: {e}\")\n",
    "    else:\n",
    "        for line in content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    yield json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON object: {e}\")\n",
    "                    print(f\"Problematic line: {line}\")\n",
    "                    continue\n",
    "\n",
    "def preprocess_function(function_text, max_seq_length=512):\n",
    "    lines = function_text.split('\\n')\n",
    "    declaration = lines[0]\n",
    "    body = '\\n'.join(lines[1:])\n",
    "    \n",
    "    func_name = declaration.split('(')[0].split()[-1]\n",
    "    \n",
    "    tokens = tokenizer.encode(body, add_special_tokens=True)\n",
    "    \n",
    "    sequences = [tokens[i:i+max_seq_length] for i in range(0, len(tokens), max_seq_length)]\n",
    "    if len(sequences[-1]) < max_seq_length:\n",
    "        sequences[-1] = sequences[-1] + [tokenizer.pad_token_id] * (max_seq_length - len(sequences[-1]))\n",
    "    \n",
    "    return np.array(sequences, dtype=np.int32), func_name\n",
    "\n",
    "def create_dataset(json_file_path, batch_size=1):\n",
    "    def gen():\n",
    "        for func_obj in read_functions_from_json(json_file_path):\n",
    "            function_text = func_obj.get('func')\n",
    "            if function_text is None:\n",
    "                print(f\"Skipping object, no function text found: {func_obj}\")\n",
    "                continue\n",
    "            sequences, name = preprocess_function(function_text)\n",
    "            if sequences is None or name is None:\n",
    "                print(f\"Skipping, preprocessed data is None for: {function_text}\")\n",
    "                continue\n",
    "            yield tf.cast(sequences, tf.int32), tf.cast(tokenizer.encode(name, add_special_tokens=True), tf.int32)\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 512), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        )\n",
    "    ).padded_batch(batch_size)\n",
    "\n",
    "# Create the dataset\n",
    "json_file_path = 'output_chunks/chunk_1.json'\n",
    "dataset = create_dataset(json_file_path)\n",
    "\n",
    "# Check the shape of the dataset\n",
    "element_spec = dataset.element_spec\n",
    "print(\"Input shape:\", element_spec[0].shape)\n",
    "print(\"Target shape:\", element_spec[1].shape)\n",
    "\n",
    "# Try to get the first batch\n",
    "try:\n",
    "    for batch in dataset.take(1):\n",
    "        inputs, targets = batch\n",
    "        print(\"Actual input shape:\", inputs.shape)\n",
    "        print(inputs)\n",
    "        print(\"Actual target shape:\", targets.shape)\n",
    "        print(targets)\n",
    "except Exception as e:\n",
    "    print(f\"Error when trying to get the first batch: {e}\")\n",
    "\n",
    "num_batches = sum(1 for _ in dataset)\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "\n",
    "class SequenceEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.lstm1 = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(lstm_units, return_sequences=False)\n",
    "        self.dense = tf.keras.layers.Dense(lstm_units, activation='relu')\n",
    "        self.lstm_units = lstm_units\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        return self.dense(x)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.lstm_units)\n",
    "\n",
    "def build_hierarchical_lstm_model(vocab_size, max_seq_length, embedding_dim=256, lstm_units=256):\n",
    "    function_input = tf.keras.layers.Input(shape=(None, max_seq_length), dtype=tf.int32)\n",
    "    \n",
    "    sequence_encoder = SequenceEncoder(vocab_size, embedding_dim, lstm_units)\n",
    "    \n",
    "    encoded_sequences = tf.keras.layers.TimeDistributed(sequence_encoder)(function_input)\n",
    "    \n",
    "    function_lstm = tf.keras.layers.LSTM(lstm_units)(encoded_sequences)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(vocab_size, activation='softmax')(function_lstm)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=function_input, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_seq_length = 512\n",
    "model = build_hierarchical_lstm_model(vocab_size, max_seq_length)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "history = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1703 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, None, 512)\n",
      "Target shape: (None, None)\n",
      "Actual input shape: (1, 4, 512)\n",
      "tf.Tensor(\n",
      "[[[    0 50117 50117 ...  1437  1437   403]\n",
      "  [  230  3808 21770 ... 38323  5457 31245]\n",
      "  [ 6972 10463  1215 ...  1437  1437  1437]\n",
      "  [48565 23770  2562 ...     1     1     1]]], shape=(1, 4, 512), dtype=int32)\n",
      "Actual target shape: (1, 13)\n",
      "tf.Tensor(\n",
      "[[    0  1215 16993  1182  6634  1215   438 38914 29015   176 11828 10887\n",
      "      2]], shape=(1, 13), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def read_functions_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove any leading/trailing whitespace\n",
    "    content = content.strip()\n",
    "    \n",
    "    # Check if the content starts and ends with square brackets\n",
    "    if content.startswith('[') and content.endswith(']'):\n",
    "        # Treat as a JSON array\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "            for item in data:\n",
    "                yield item\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON array: {e}\")\n",
    "    else:\n",
    "        # Treat as individual JSON objects, one per line\n",
    "        for line in content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    yield json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON object: {e}\")\n",
    "                    print(f\"Problematic line: {line}\")\n",
    "                    continue\n",
    "\n",
    "def preprocess_function(function_text, max_seq_length=512):\n",
    "    # Split the function declaration (assumed to be the first line) from the body\n",
    "    lines = function_text.split('\\n')\n",
    "    declaration = lines[0]\n",
    "    body = '\\n'.join(lines[1:])\n",
    "    \n",
    "    # Extract function name from declaration\n",
    "    func_name = declaration.split('(')[0].split()[-1]\n",
    "    # print(\"body\", body)\n",
    "    # Tokenize the function body\n",
    "    tokens = tokenizer.encode(body, add_special_tokens=True)\n",
    "    # print(\"tokens\", tokens)\n",
    "    # print(\"shape of tokens\", len(tokens))\n",
    "    # Split into sequences of max_seq_length\n",
    "    sequences = [tokens[i:i+max_seq_length] for i in range(0, len(tokens), max_seq_length)]\n",
    "    # Pad the last sequence if necessary\n",
    "    if len(sequences[-1]) < max_seq_length:\n",
    "        sequences[-1] = sequences[-1] + [tokenizer.pad_token_id] * (max_seq_length - len(sequences[-1]))\n",
    "    \n",
    "    return np.array(sequences, dtype=np.int32), func_name\n",
    "\n",
    "def create_dataset(json_file_path, batch_size=1):\n",
    "    def gen():\n",
    "        for func_obj in read_functions_from_json(json_file_path):\n",
    "            function_text = func_obj.get('func')\n",
    "            if function_text is None:\n",
    "                print(f\"Skipping object, no function text found: {func_obj}\")\n",
    "                continue\n",
    "            sequences, name = preprocess_function(function_text)\n",
    "            # print(\"name\", name, tokenizer.encode(name, add_special_tokens=True))\n",
    "            if sequences is None or name is None:\n",
    "                print(f\"Skipping, preprocessed data is None for: {function_text}\")\n",
    "                continue\n",
    "            # yield sequences, tokenizer.encode(name, add_special_tokens=True)\n",
    "            yield tf.cast(sequences, tf.int32), tf.cast(tokenizer.encode(name, add_special_tokens=True), tf.int32)\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 512), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        )\n",
    "    ).padded_batch(batch_size)\n",
    "\n",
    "# Create the dataset\n",
    "# json_file_path = 'output_chunks/chunk_3305.json'\n",
    "json_file_path = 'output_chunks/chunk_1.json'\n",
    "\n",
    "dataset = create_dataset(json_file_path)\n",
    "\n",
    "# Check the shape of the dataset\n",
    "element_spec = dataset.element_spec\n",
    "print(\"Input shape:\", element_spec[0].shape)\n",
    "print(\"Target shape:\", element_spec[1].shape)\n",
    "\n",
    "# Try to get the first batch\n",
    "try:\n",
    "    for batch in dataset.take(1):\n",
    "        inputs, targets = batch\n",
    "        print(\"Actual input shape:\", inputs.shape)\n",
    "        print(inputs)\n",
    "        print(\"Actual target shape:\", targets.shape)\n",
    "        print(targets)\n",
    "except Exception as e:\n",
    "    print(f\"Error when trying to get the first batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 100\n"
     ]
    }
   ],
   "source": [
    "num_batches = sum(1 for _ in dataset)\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinay\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py:360: UserWarning: `build()` was called on layer 'sequence_encoder_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_13                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50265</span>)               │      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,918,105</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_13                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_38 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m525,312\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50265\u001b[0m)               │      \u001b[38;5;34m12,918,105\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,443,417</span> (51.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,443,417\u001b[0m (51.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,443,417</span> (51.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,443,417\u001b[0m (51.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class SequenceEncoder(layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.lstm1 = layers.LSTM(lstm_units, return_sequences=True)\n",
    "        self.lstm2 = layers.LSTM(lstm_units, return_sequences=False)\n",
    "        self.dense = layers.Dense(lstm_units, activation='relu')\n",
    "        self.lstm_units = lstm_units\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        return self.dense(x)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.lstm_units)\n",
    "\n",
    "def build_hierarchical_lstm_model(vocab_size, max_seq_length, embedding_dim=256, lstm_units=256):\n",
    "    # Input layer for multiple sequences (variable number of sequences per function)\n",
    "    # function_input = layers.Input(shape=(None, max_seq_length))\n",
    "    function_input = tf.keras.layers.Input(shape=(None, max_seq_length), dtype=tf.int32)\n",
    "    \n",
    "    # Sequence encoder layer\n",
    "    sequence_encoder = SequenceEncoder(vocab_size, embedding_dim, lstm_units)\n",
    "    \n",
    "    # Apply the sequence encoder to each sequence in the function\n",
    "    encoded_sequences = layers.TimeDistributed(sequence_encoder)(function_input)\n",
    "    \n",
    "    # LSTM layer to process all encoded sequences\n",
    "    function_lstm = layers.LSTM(lstm_units)(encoded_sequences)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(vocab_size, activation='softmax')(function_lstm)\n",
    "    \n",
    "    # Full model\n",
    "    model = Model(inputs=function_input, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_seq_length = 512\n",
    "model = build_hierarchical_lstm_model(vocab_size, max_seq_length)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mInvalid dtype: NoneType\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, None, 512), dtype=int32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Adjust based on your dataset size\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(dataset, epochs\u001b[38;5;241m=\u001b[39mepochs, steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\variables.py:495\u001b[0m, in \u001b[0;36mstandardize_dtype\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m    492\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mALLOWED_DTYPES:\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mInvalid dtype: NoneType\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, None, 512), dtype=int32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming your dataset is already prepared and called 'dataset'\n",
    "\n",
    "# # Training parameters\n",
    "# epochs = 10\n",
    "# steps_per_epoch = 100  # Adjust based on your dataset size\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# print(\"Training completed.\")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10\n",
    "steps_per_epoch = 100  # Adjust based on your dataset size\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to predict function name\n",
    "def predict_function_name(model, function_body):\n",
    "    sequences, _ = preprocess_function(function_body)\n",
    "    sequences = tf.expand_dims(sequences, axis=0)  # Add batch dimension\n",
    "    predicted_tokens = model.predict(sequences)\n",
    "    \n",
    "    # Get the index of the most likely token for each position\n",
    "    predicted_indices = np.argmax(predicted_tokens, axis=-1)[0]\n",
    "    \n",
    "    # Decode the indices to get the predicted name\n",
    "    predicted_name = tokenizer.decode(predicted_indices)\n",
    "    \n",
    "    # Remove any special tokens and whitespace\n",
    "    predicted_name = predicted_name.strip().replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "    \n",
    "    return predicted_name\n",
    "\n",
    "# Example usage\n",
    "function_body = \"\"\"\n",
    "def example_function(x, y):\n",
    "    result = x + y\n",
    "    print(f\"The sum of {x} and {y} is {result}\")\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "predicted_name = predict_function_name(model, function_body)\n",
    "print(f\"Function body:\\n{function_body}\")\n",
    "print(f\"Predicted function name: {predicted_name}\")\n",
    "\n",
    "# Try a few more examples\n",
    "more_examples = [\n",
    "    \"\"\"\n",
    "    def calculate_average(numbers):\n",
    "        total = sum(numbers)\n",
    "        count = len(numbers)\n",
    "        return total / count if count > 0 else 0\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def is_prime(n):\n",
    "        if n < 2:\n",
    "            return False\n",
    "        for i in range(2, int(n**0.5) + 1):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, example in enumerate(more_examples, 1):\n",
    "    predicted_name = predict_function_name(model, example)\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Function body:\\n{example}\")\n",
    "    print(f\"Predicted function name: {predicted_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
