{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def split_json_file(input_file, output_dir, chunk_size):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        chunk_number = 1\n",
    "        while True:\n",
    "            # Read lines in chunks\n",
    "            lines = [file.readline() for _ in range(chunk_size)]\n",
    "            if not any(lines):\n",
    "                break  # Exit loop if no more lines\n",
    "\n",
    "            # Filter out empty lines\n",
    "            lines = [line for line in lines if line.strip()]\n",
    "\n",
    "            # Parse each line as JSON and write to a new file\n",
    "            data = [json.loads(line) for line in lines]\n",
    "            output_file = os.path.join(output_dir, f'chunk_{chunk_number}.json')\n",
    "            with open(output_file, 'w') as out_file:\n",
    "                json.dump(data, out_file, indent=4)\n",
    "            \n",
    "            chunk_number += 1\n",
    "\n",
    "# Example usag\n",
    "input_file = 'function_data.json'\n",
    "output_dir = 'output_chunks'\n",
    "chunk_size = 100  # Adjust the chunk size as needed\n",
    "\n",
    "split_json_file(input_file, output_dir, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFRobertaModel, RobertaTokenizer\n",
    "\n",
    "class CodeProcessor:\n",
    "    def __init__(self, max_segment_length=512):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "        self.code_model = TFRobertaModel.from_pretrained('microsoft/codebert-base')\n",
    "        self.max_segment_length = max_segment_length\n",
    "\n",
    "    def preprocess(self, raw_code: str) -> Tuple[str, str]:\n",
    "        # Extract function name and body\n",
    "        match = re.search(r'(\\w+)\\s*\\([^)]*\\)\\s*{([\\s\\S]*)}', raw_code)\n",
    "        if not match:\n",
    "            raise ValueError(\"Invalid function format\")\n",
    "        function_name, function_body = match.groups()\n",
    "        function_body = function_body.strip()\n",
    "        return function_name, function_body\n",
    "\n",
    "    def segment_code(self, function_body: str) -> List[str]:\n",
    "        lines = function_body.split('\\n')\n",
    "        segments = []\n",
    "        current_segment = []\n",
    "        current_length = 0\n",
    "\n",
    "        for line in lines:\n",
    "            line_tokens = self.tokenizer.tokenize(line)\n",
    "            if current_length + len(line_tokens) > self.max_segment_length:\n",
    "                segments.append('\\n'.join(current_segment))\n",
    "                current_segment = [line]\n",
    "                current_length = len(line_tokens)\n",
    "            else:\n",
    "                current_segment.append(line)\n",
    "                current_length += len(line_tokens)\n",
    "\n",
    "        if current_segment:\n",
    "            segments.append('\\n'.join(current_segment))\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def embed_segments(self, segments: List[str]) -> tf.Tensor:\n",
    "        embeddings = []\n",
    "        for segment in segments:\n",
    "            inputs = self.tokenizer(segment, return_tensors='tf', max_length=self.max_segment_length, \n",
    "                                    truncation=True, padding='max_length')\n",
    "            output = self.code_model(inputs)[0]\n",
    "            embeddings.append(output[:, 0, :])  # Use [CLS] token representation\n",
    "        return tf.concat(embeddings, axis=0)\n",
    "\n",
    "class HierarchicalCodeModel(keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        self.segment_encoder = keras.layers.LSTM(hidden_dim, return_sequences=True)\n",
    "        self.attention = keras.layers.MultiHeadAttention(num_heads=4, key_dim=hidden_dim)\n",
    "        self.fc = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch_size, num_segments, input_dim)\n",
    "        encoded = self.segment_encoder(inputs)\n",
    "        attended = self.attention(encoded, encoded)\n",
    "        pooled = tf.reduce_mean(attended, axis=1)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "class FunctionNameGenerator:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        self.processor = CodeProcessor()\n",
    "        self.hierarchical_model = HierarchicalCodeModel(768, hidden_dim, num_layers, embedding_dim)\n",
    "        self.name_generator = keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def generate_name(self, raw_code: str) -> str:\n",
    "        # Preprocess\n",
    "        _, function_body = self.processor.preprocess(raw_code)\n",
    "\n",
    "        # Segment\n",
    "        segments = self.processor.segment_code(function_body)\n",
    "\n",
    "        # Embed\n",
    "        segment_embeddings = self.processor.embed_segments(segments)\n",
    "\n",
    "        # Hierarchical processing\n",
    "        function_embedding = self.hierarchical_model(tf.expand_dims(segment_embeddings, axis=0))\n",
    "\n",
    "        # Generate name (simplified here - in practice, you'd use a more sophisticated generation method)\n",
    "        name_logits = self.name_generator(function_embedding)\n",
    "        name_tokens = tf.argmax(name_logits, axis=-1)\n",
    "\n",
    "        # Convert tokens to string (simplified - you'd need to implement token-to-string conversion)\n",
    "        generated_name = self.tokens_to_name(name_tokens)\n",
    "\n",
    "        return generated_name\n",
    "\n",
    "    def tokens_to_name(self, tokens):\n",
    "        # Implement conversion from tokens to string\n",
    "        # This is a placeholder - you'll need to implement this based on your tokenization scheme\n",
    "        return \"generated_function_name\"\n",
    "\n",
    "# Usage\n",
    "generator = FunctionNameGenerator(vocab_size=10000, embedding_dim=256, hidden_dim=512, num_layers=2)\n",
    "\n",
    "# Compile the model\n",
    "generator.hierarchical_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Example usage\n",
    "your_raw_code = \"\"\"\n",
    "def example_function(arg1, arg2):\n",
    "    # Function body here\n",
    "    result = arg1 + arg2\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "function_name = generator.generate_name(your_raw_code)\n",
    "print(f\"Generated function name: {function_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
